#include <bits/stdc++.h>
using namespace std;

// ---------- Read CSV File ----------
vector<vector<string>> readCSV(const string &filename) {
    vector<vector<string>> data;
    ifstream file(filename);
    if (!file.is_open()) {
        cerr << "Error: Cannot open file " << filename << endl;
        return data;
    }

    string line;
    while (getline(file, line)) {
        if (line.empty()) continue;
        stringstream ss(line);
        string cell;
        vector<string> row;
        while (getline(ss, cell, ',')) {
            // Remove extra spaces
            while (!cell.empty() && cell.front() == ' ') cell.erase(cell.begin());
            while (!cell.empty() && cell.back() == ' ') cell.pop_back();
            row.push_back(cell);
        }
        data.push_back(row);
    }
    file.close();
    return data;
}

// ---------- Check if String is Numeric ----------
bool isNumeric(const string &s) {
    if (s.empty()) return false;
    try {
        stod(s);
        return true;
    } catch (...) {
        return false;
    }
}

// ---------- Main ----------
int main() {
    ios::sync_with_stdio(false);
    cin.tie(nullptr);

    string filename;
    cout << "Enter CSV filename (e.g. data.csv): " << flush;
    getline(cin >> ws, filename); // handles spaces properly

    vector<vector<string>> data = readCSV(filename);
    if (data.empty()) {
        cerr << "Error: Empty or invalid CSV file.\n";
        return 1;
    }

    vector<string> headers = data[0];
    cout << "\nColumns detected:\n";
    for (size_t i = 0; i < headers.size(); ++i)
        cout << i + 1 << ". " << headers[i] << endl;

    int colX, colY;
    cout << "\nSelect column number for X: " << flush;
    cin >> colX;
    cout << "Select column number for Y: " << flush;
    cin >> colY;

    colX--; colY--; // Convert 1-based index to 0-based
    if (colX >= headers.size() || colY >= headers.size()) {
        cerr << "Error: Invalid column numbers.\n";
        return 1;
    }

    vector<double> x, y;
    for (size_t i = 1; i < data.size(); ++i) {
        if (colX < data[i].size() && colY < data[i].size()) {
            string sx = data[i][colX];
            string sy = data[i][colY];
            if (isNumeric(sx) && isNumeric(sy)) {
                x.push_back(stod(sx));
                y.push_back(stod(sy));
            }
        }
    }

    int n = x.size();
    if (n == 0) {
        cerr << "Error: No valid numeric data found in selected columns.\n";
        return 1;
    }

    // ---------- Intermediate Steps ----------
    double sumx = accumulate(x.begin(), x.end(), 0.0);
    double sumy = accumulate(y.begin(), y.end(), 0.0);
    double sumxy = 0, sumx2 = 0;
    for (int i = 0; i < n; i++) {
        sumxy += x[i] * y[i];
        sumx2 += x[i] * x[i];
    }

    cout << "\n---------- Intermediate Calculations ----------\n";
    cout << "Î£X = " << sumx << ", Î£Y = " << sumy << "\n";
    cout << "Î£XY = " << sumxy << ", Î£XÂ² = " << sumx2 << "\n";
    cout << "n = " << n << "\n";

    // ---------- Regression Equation ----------
    double b1 = (n * sumxy - sumx * sumy) / (n * sumx2 - sumx * sumx);
    double b0 = (sumy - b1 * sumx) / n;

    cout << fixed << setprecision(4);
    cout << "\n========== Linear Regression Result ==========\n";
    cout << "Selected Columns: X = " << headers[colX]
         << ", Y = " << headers[colY] << "\n";
    cout << "Regression Equation: Y = " << b0 << " + " << b1 << " * X\n";

    // ---------- Prediction ----------
    double x_pred;
    cout << "\nEnter X to predict Y: " << flush;
    cin >> x_pred;
    double y_pred = b0 + b1 * x_pred;
    cout << "Predicted Y = " << y_pred << "\n";

    cout << "==============================================\n";
    return 0;
}

// ==================================================================================================
// ðŸ”¹ DETAILED EXPLANATION OF SIMPLE LINEAR REGRESSION PROGRAM
// ==================================================================================================
//
// ðŸ§© PURPOSE:
// This program implements **Simple Linear Regression (SLR)** â€” one of the fundamental techniques
// in **Prediction and Data Mining**.  
// It establishes a **mathematical relationship between two numeric variables (X and Y)**,
// allowing prediction of Y for any given value of X.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 1ï¸âƒ£ THEORY OF LINEAR REGRESSION
// --------------------------------------------------------------------------------------------------
//
// âž¤ Objective:
// To find a straight line (best fit line) that models the relationship between dependent variable (Y)
// and independent variable (X).
//
// âž¤ Equation of Regression Line:
//     Y = bâ‚€ + bâ‚ * X
//
// where:
//     bâ‚€ = Intercept  (value of Y when X = 0)
//     bâ‚ = Slope of regression line (change in Y per unit change in X)
//
// âž¤ The slope (bâ‚) and intercept (bâ‚€) are computed using the **Least Squares Method**:
//
//     bâ‚ = [ n(Î£XY) - (Î£X)(Î£Y) ] / [ n(Î£XÂ²) - (Î£X)Â² ]
//     bâ‚€ = (Î£Y - bâ‚Î£X) / n
//
// The line minimizes the sum of squared vertical distances between observed values and predicted values.
//
// âž¤ Prediction:
// Once bâ‚€ and bâ‚ are known, Y for any input X can be predicted using:
//     Å¶ = bâ‚€ + bâ‚ * X
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 2ï¸âƒ£ FUNCTION OVERVIEW
// --------------------------------------------------------------------------------------------------
//
// âž¤ readCSV()
//     - Reads the input CSV dataset line by line.
//     - Splits each row by commas.
//     - Removes leading/trailing spaces and stores the data in a 2D vector.
//
// âž¤ isNumeric()
//     - Checks whether a string represents a valid numeric value (integer or decimal).
//     - Helps skip non-numeric data in chosen columns.
//
// âž¤ main()
//     - Handles overall workflow:
//         1ï¸âƒ£ Reads CSV file
//         2ï¸âƒ£ Displays available columns
//         3ï¸âƒ£ User selects X and Y columns
//         4ï¸âƒ£ Extracts numeric values
//         5ï¸âƒ£ Computes regression coefficients (bâ‚€, bâ‚)
//         6ï¸âƒ£ Prints regression equation
//         7ï¸âƒ£ Predicts Y for a given X
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 3ï¸âƒ£ STEP-BY-STEP EXECUTION
// --------------------------------------------------------------------------------------------------
//
// STEP 1ï¸âƒ£ â†’ INPUT DATA
//     - User provides a CSV file (e.g., `student_scores.csv`).
//     - Program displays all columns and allows user to select which will act as:
//         X = independent variable
//         Y = dependent variable
//
// Example CSV:
//     Hours, Marks
//     1, 50
//     2, 55
//     3, 65
//     4, 70
//     5, 80
//
// User chooses: X = Hours, Y = Marks
//
// STEP 2ï¸âƒ£ â†’ INTERMEDIATE SUMMATIONS
//     The program computes the following intermediate values:
//
//     Î£X   = sum of all X values
//     Î£Y   = sum of all Y values
//     Î£XY  = sum of product of X and Y
//     Î£XÂ²  = sum of squares of X
//     n    = number of records
//
// Example Output:
//     Î£X = 15, Î£Y = 320
//     Î£XY = 1050, Î£XÂ² = 55, n = 5
//
// STEP 3ï¸âƒ£ â†’ COMPUTE REGRESSION PARAMETERS
//
//     bâ‚ = [ n(Î£XY) - (Î£X)(Î£Y) ] / [ n(Î£XÂ²) - (Î£X)Â² ]
//     bâ‚€ = (Î£Y - bâ‚Î£X) / n
//
// Example Calculation:
//     bâ‚ = [5(1050) - 15(320)] / [5(55) - 15Â²] = (5250 - 4800) / (275 - 225) = 450 / 50 = 9
//     bâ‚€ = (320 - 9(15)) / 5 = (320 - 135) / 5 = 185 / 5 = 37
//
// Therefore, regression equation is:
//     Y = 37 + 9X
//
// STEP 4ï¸âƒ£ â†’ PREDICTION
//     User enters a test value for X (e.g., X = 6).
//     The program predicts:
//         Y = 37 + 9 * 6 = 91
//
//     Output:
//         Predicted Y = 91
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 4ï¸âƒ£ EXAMPLE OUTPUT
// --------------------------------------------------------------------------------------------------
//
// Enter CSV filename (e.g. data.csv): student_scores.csv
//
// Columns detected:
// 1. Hours
// 2. Marks
//
// Select column number for X: 1
// Select column number for Y: 2
//
// ---------- Intermediate Calculations ----------
// Î£X = 15, Î£Y = 320
// Î£XY = 1050, Î£XÂ² = 55
// n = 5
//
// ========== Linear Regression Result ==========
// Selected Columns: X = Hours, Y = Marks
// Regression Equation: Y = 37.0000 + 9.0000 * X
//
// Enter X to predict Y: 6
// Predicted Y = 91.0000
// ==============================================
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 5ï¸âƒ£ VARIABLES USED
// --------------------------------------------------------------------------------------------------
//
// filename         â†’ input CSV filename
// data[][]         â†’ 2D vector containing dataset rows
// headers[]        â†’ column names (from first row)
// colX, colY       â†’ selected column indices for X and Y
// x[], y[]         â†’ numeric values extracted from dataset
// sumx, sumy       â†’ summations Î£X and Î£Y
// sumxy, sumx2     â†’ Î£XY and Î£XÂ² respectively
// b0, b1           â†’ regression coefficients (intercept, slope)
// x_pred, y_pred   â†’ user input for prediction and predicted value
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 6ï¸âƒ£ WHY LINEAR REGRESSION WAS USED (JUSTIFICATION)
// --------------------------------------------------------------------------------------------------
//
// ðŸ”¹ Dataset Type:
//     - Contains two **numeric attributes** with a potential **linear relationship**.
//
// ðŸ”¹ Objective:
//     - To **predict** the dependent variable (Y) based on the independent variable (X).
//
// ðŸ”¹ Why Simple Linear Regression:
//     1ï¸âƒ£ It identifies the **best-fit line** through data points.
//     2ï¸âƒ£ Provides an easy-to-understand mathematical model for prediction.
//     3ï¸âƒ£ Useful for **forecasting and trend analysis** in numeric datasets.
//     4ï¸âƒ£ Forms the foundation for more complex models (multiple regression, polynomial regression).
//
// ðŸ”¹ Applications:
//     - Predicting student marks from study hours
//     - Estimating sales from advertising spend
//     - Predicting temperature, profit, or growth based on linear trends
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 7ï¸âƒ£ ADVANTAGES
// --------------------------------------------------------------------------------------------------
//
// âœ… Simple to implement and interpret.
// âœ… Provides clear cause-effect relationship between X and Y.
// âœ… Useful for both explanatory and predictive modeling.
// âœ… Performs well when data shows a linear trend.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 8ï¸âƒ£ LIMITATIONS
// --------------------------------------------------------------------------------------------------
//
// âš ï¸ Assumes linear relationship â€” not suitable for non-linear data.
// âš ï¸ Sensitive to outliers, which can distort regression coefficients.
// âš ï¸ Works only for numeric data.
// âš ï¸ Correlation does not imply causation â€” linear relation doesnâ€™t always mean dependency.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 9ï¸âƒ£ CONCLUSION
// --------------------------------------------------------------------------------------------------
//
// âž¤ The Simple Linear Regression model successfully computes the relationship between two numeric
//     variables using the **Least Squares Method**.
//
// âž¤ The equation derived (Y = bâ‚€ + bâ‚X) can predict future or unknown Y values for any given X.
//
// âž¤ Example Conclusion:
//     â€œBased on regression analysis, the dependent variable Y increases linearly with X.
//      The best-fit regression line obtained can be used for accurate numeric prediction.â€
//
// --------------------------------------------------------------------------------------------------
// âœ… FINAL REMARK:
// This experiment demonstrates **Prediction in Data Mining** using the **Simple Linear Regression**
// technique. It helps discover linear relationships between numeric attributes and predict outcomes
// based on them, forming the basis for more advanced predictive analytics.
//
// ==================================================================================================
