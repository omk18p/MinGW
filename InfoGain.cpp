#include <bits/stdc++.h>
using namespace std;

double entropy(const map<string, double> &classCounts) {
    double total = 0.0;
    for (auto &p : classCounts) total += p.second;
    if (total == 0) return 0.0;

    double e = 0.0;
    for (auto &p : classCounts) {
        double ratio = p.second / total;
        if (ratio > 0)
            e += -ratio * log2(ratio);
    }
    return e;
}

vector<vector<string>> readCSV(const string &filename) {
    vector<vector<string>> data;
    ifstream file(filename);
    string line;
    while (getline(file, line)) {
        if (line.empty()) continue;
        stringstream ss(line);
        string cell;
        vector<string> row;
        while (getline(ss, cell, ',')) {
            if (!cell.empty() && cell.front() == ' ') cell.erase(cell.begin());
            if (!cell.empty() && cell.back() == ' ') cell.pop_back();
            row.push_back(cell);
        }
        data.push_back(row);
    }
    return data;
}

int main() {
    string filename;
    cout << "Enter CSV filename: ";
    cin >> filename;

    vector<vector<string>> data = readCSV(filename);
    if (data.empty()) {
        cout << "Error: File empty or invalid.\n";
        return 0;
    }

    vector<string> headers = data[0];
    int totalRecords = data.size() - 1;
    string target = headers.back();

    cout << fixed << setprecision(4);

    // Step 1: Calculate Parent Entropy
    map<string, double> totalTargetCount;
    for (int i = 1; i < data.size(); i++)
        totalTargetCount[data[i].back()]++;

    double totalEntropy = entropy(totalTargetCount);
    cout << "\nEntropy (Parent) = " << totalEntropy << endl;

    string bestAttr;
    double bestInfoGain = -1;

    // Step 2: For each attribute, calculate Information Gain
    for (int col = 0; col < headers.size() - 1; col++) {
        string attr = headers[col];
        cout << "\nFor Attribute: " << attr << endl;

        // Count how target classes distribute under each attribute value
        map<string, map<string, double>> valueClassCount;
        for (int i = 1; i < data.size(); i++)
            valueClassCount[data[i][col]][data[i].back()]++;

        double weightedEntropy = 0.0;

        for (auto &kv : valueClassCount) {
            string value = kv.first;
            double subsetTotal = 0.0;
            for (auto &cls : kv.second) subsetTotal += cls.second;

            double e = entropy(kv.second);
            weightedEntropy += (subsetTotal / totalRecords) * e;

            cout << "  Entropy(" << value << ") = " << e << " -> ";
            for (auto &cls : kv.second)
                cout << cls.first << "=" << cls.second << " ";
            cout << endl;
        }

        double infoGain = totalEntropy - weightedEntropy;
        cout << "  Information Gain (" << attr << ") = " << infoGain << endl;
        cout << "----------------------------------------" << endl;

        if (infoGain > bestInfoGain) {
            bestInfoGain = infoGain;
            bestAttr = attr;
        }
    }

    cout << "\nBest Attribute = " << bestAttr << "  (Gain = " << bestInfoGain << ")\n";
    return 0;
}
// ==================================================================================================
// üîπ DETAILED EXPLANATION OF ENTROPY AND INFORMATION GAIN CALCULATION PROGRAM
// ==================================================================================================
//
// üß© PURPOSE:
// This program demonstrates **Entropy** and **Information Gain** computation, which are the
// core mathematical concepts behind the **ID3 Decision Tree algorithm**.
// 
// The goal is to determine **which attribute best splits the dataset** based on information gain ‚Äî
// that is, how much "uncertainty" or "disorder" in the target class is reduced by using that attribute.
//
// --------------------------------------------------------------------------------------------------
// üî∏ 1Ô∏è‚É£ FUNCTION OVERVIEW
// --------------------------------------------------------------------------------------------------
//
// ‚û§ entropy()
//     - Calculates the **entropy** of a given class distribution.
//     - Entropy measures impurity or randomness in the data.
//     - Formula:  Entropy(S) = ‚àí ‚àë (pi * log‚ÇÇ(pi))
//         where pi = proportion of class i in the dataset.
//     - Example:
//           If 4 samples are ‚ÄúYes‚Äù and 4 are ‚ÄúNo‚Äù
//           ‚Üí pi(Yes) = 0.5, pi(No) = 0.5
//           ‚Üí Entropy = ‚àí[0.5*log‚ÇÇ(0.5) + 0.5*log‚ÇÇ(0.5)] = 1.0
//       (Maximum uncertainty)
//
// ‚û§ readCSV()
//     - Reads the dataset from a CSV file.
//     - Stores each row as a vector<string>.
//     - The first row (headers) contains attribute names.
//     - The last column is the **target attribute** (class label).
//
// ‚û§ main()
//     - Reads the dataset.
//     - Calculates total (parent) entropy of the target class.
//     - For each attribute, calculates the **weighted child entropies** and **Information Gain**.
//     - Selects and displays the attribute with the **highest information gain** ‚Äî
//       the best attribute for splitting in a decision tree.
//
// --------------------------------------------------------------------------------------------------
// üî∏ 2Ô∏è‚É£ STEP-BY-STEP LOGICAL FLOW
// --------------------------------------------------------------------------------------------------
//
// STEP 1Ô∏è‚É£ ‚Üí Input and Initialization
//     - User enters CSV filename (e.g., ‚Äúplay_tennis.csv‚Äù).
//     - Data is read into a 2D vector ‚Äúdata‚Äù.
//     - Headers are extracted from the first row.
//     - The target class is assumed to be the **last column**.
//
// STEP 2Ô∏è‚É£ ‚Üí Calculate Parent Entropy
//     - Count how many records belong to each target class (e.g., Yes/No).
//     - Pass these counts to entropy() to compute parent entropy (total disorder).
//     - Example Output:
//           Entropy (Parent) = 0.9403
//
// STEP 3Ô∏è‚É£ ‚Üí Attribute-wise Entropy Calculation
//     - For each attribute (except target):
//         ‚ñ™ Group records based on attribute value (e.g., Outlook = Sunny, Overcast, Rainy).
//         ‚ñ™ For each group, calculate entropy of the target class distribution.
//         ‚ñ™ Multiply each subset‚Äôs entropy by its proportion in the total dataset.
//     - Example:
//           For Outlook:
//               Sunny ‚Üí Entropy = 0.971
//               Overcast ‚Üí Entropy = 0.000
//               Rainy ‚Üí Entropy = 0.971
//
// STEP 4Ô∏è‚É£ ‚Üí Compute Weighted Entropy & Information Gain
//     - Weighted Entropy = ‚àë (subset_size / total_size) * Entropy(subset)
//     - Information Gain = Parent Entropy ‚àí Weighted Entropy
//     - The attribute with the **highest Information Gain** gives the purest split.
//
// STEP 5Ô∏è‚É£ ‚Üí Display Results
//     - Prints each attribute‚Äôs entropy and gain.
//     - Displays the **best attribute** for decision tree root selection.
//
// Example Output:
//
//     Entropy (Parent) = 0.9403
//
//     For Attribute: Outlook
//       Entropy(Sunny) = 0.9710 -> Yes=2 No=3
//       Entropy(Overcast) = 0.0000 -> Yes=4
//       Entropy(Rainy) = 0.9710 -> Yes=3 No=2
//       Information Gain (Outlook) = 0.246
//     ----------------------------------------
//
//     For Attribute: Humidity
//       Entropy(High) = 1.0000 -> Yes=3 No=3
//       Entropy(Normal) = 0.0000 -> Yes=6
//       Information Gain (Humidity) = 0.151
//
//     Best Attribute = Outlook (Gain = 0.246)
//
// --------------------------------------------------------------------------------------------------
// üî∏ 3Ô∏è‚É£ INTERNAL VARIABLE DESCRIPTION
// --------------------------------------------------------------------------------------------------
//
// data[][]          ‚Üí Stores dataset read from CSV.
// headers[]         ‚Üí Stores attribute names from first row.
// totalRecords      ‚Üí Number of data records (excluding header).
// target            ‚Üí Name of target class column.
// totalTargetCount  ‚Üí Map storing frequency of each class label (e.g., Yes=9, No=5).
// valueClassCount   ‚Üí Nested map to count occurrences of class labels for each attribute value.
// totalEntropy      ‚Üí Entropy of the parent dataset (before splitting).
// weightedEntropy   ‚Üí Average entropy across all attribute values, weighted by subset size.
// infoGain          ‚Üí Difference between totalEntropy and weightedEntropy (gain in purity).
//
// --------------------------------------------------------------------------------------------------
// üî∏ 4Ô∏è‚É£ WHY ENTROPY AND INFORMATION GAIN ARE USED
// --------------------------------------------------------------------------------------------------
//
// üîπ Entropy measures **disorder** in the dataset.
//     - High entropy ‚Üí data is mixed and impure.
//     - Low entropy ‚Üí data is more pure (closer to one class).
//
// üîπ Information Gain quantifies **how much entropy is reduced** by splitting on an attribute.
//     - High information gain ‚Üí attribute provides more useful information.
//     - The attribute with maximum gain is chosen for the **root** or **internal split** in the tree.
//
// üîπ Example Intuition:
//     - If ‚ÄúOutlook = Overcast‚Äù always results in ‚ÄúPlay = Yes‚Äù, entropy = 0 ‚Üí perfectly pure branch.
//     - If ‚ÄúOutlook = Sunny‚Äù results in a 50‚Äì50 Yes/No split ‚Üí high entropy ‚Üí less useful.
//
// --------------------------------------------------------------------------------------------------
// üî∏ 5Ô∏è‚É£ WHY THIS METHOD (ID3-STYLE ENTROPY) WAS CHOSEN
// --------------------------------------------------------------------------------------------------
//
// ‚úÖ Dataset Type:
//     - The dataset (e.g., ‚ÄúPlay_Tennis1‚Äù) has **categorical attributes** and a **categorical class label**.
//     - Perfect for **classification** tasks.
//
// ‚úÖ Reason for Choosing ID3/Entropy-Based Method:
//     1Ô∏è‚É£ Handles categorical attributes naturally.
//     2Ô∏è‚É£ Identifies the most significant attribute for classification.
//     3Ô∏è‚É£ Produces interpretable results (which attribute influences decision most).
//     4Ô∏è‚É£ Foundation for building a full **Decision Tree Classifier (ID3)**.
//
// ‚úÖ Comparison with Other Techniques:
//     - **Gini Index (CART):** Similar purpose but uses a different impurity measure (Gini instead of Entropy).
//     - **Naive Bayes:** Based on probability, not attribute-based splitting.
//     - **K-Means / DBSCAN:** Unsupervised clustering methods (no class labels).
//     - ‚úÖ **Entropy (ID3)** is ideal for categorical classification and feature selection.
//
// --------------------------------------------------------------------------------------------------
// üî∏ 6Ô∏è‚É£ CONCLUSION
// --------------------------------------------------------------------------------------------------
//
// ‚û§ This program demonstrates how **Entropy** and **Information Gain** guide the selection of
//     the best attribute for classification.
//
// ‚û§ The attribute with the **highest Information Gain** reduces uncertainty the most and forms
//     the root node in a Decision Tree.
//
// ‚û§ Example Conclusion:
//     ‚ÄúOutlook‚Äù was chosen as the root attribute since it provided the highest gain (0.246).
//
// ‚û§ Summary of Advantages:
//     - Quantifies uncertainty mathematically.
//     - Provides objective attribute ranking.
//     - Forms the foundation for **ID3 Decision Tree construction**.
//
// --------------------------------------------------------------------------------------------------
// ‚úÖ FINAL REMARK:
// This experiment demonstrates the **Information Gain and Entropy Method** ‚Äî the core principle
// behind the ID3 Decision Tree algorithm.  
// It effectively identifies the most informative attributes for classification, enabling the
// construction of accurate and interpretable decision models.
//
// ==================================================================================================
