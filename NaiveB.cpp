#include <bits/stdc++.h>
using namespace std;

// Split CSV line by delimiter
vector<string> split(const string &s, char d) {
    vector<string> tokens; string temp; stringstream ss(s);
    while (getline(ss, temp, d)) tokens.push_back(temp);
    return tokens;
}

int main() {
    string filename;
    cout << "Enter CSV filename: ";
    cin >> filename;

    ifstream file(filename);
    if (!file.is_open()) {
        cout << "Error: Cannot open file.\n";
        return 0;
    }

    string line;
    getline(file, line);
    vector<string> headers = split(line, ',');
    int featureCount = headers.size() - 1;  // Last column = Class

    vector<vector<string>> data;
    while (getline(file, line)) {
        vector<string> row = split(line, ',');
        if (row.size() == headers.size())
            data.push_back(row);
    }
    file.close();

    cout << "\n--- Dataset Loaded Successfully ---\n";
    cout << "Total records: " << data.size() << endl;
    cout << "Features: " << featureCount << endl;
    cout << "Target (Class): " << headers.back() << "\n";

    // Count class occurrences
    map<string, int> classCount;
    for (auto &r : data) classCount[r.back()]++;

    int total = data.size();
    cout << "\n=== PRIOR PROBABILITIES ===\n";
    map<string, double> prior;
    for (auto &c : classCount) {
        prior[c.first] = (double)c.second / total;
        cout << "P(" << c.first << ") = " << c.second << "/" << total
             << " = " << fixed << setprecision(3) << prior[c.first] << endl;
    }

    // Calculate conditional probabilities P(feature=value | class)
    cout << "\n=== CONDITIONAL PROBABILITIES ===\n";
    map<string, map<string, map<string, double>>> condProb;

    for (int i = 0; i < featureCount; i++) {
        map<string, map<string, int>> freq;
        for (auto &r : data) {
            freq[r.back()][r[i]]++;
        }

        for (auto &cls : classCount) {
            cout << "\nFor Class = " << cls.first
                 << " (" << headers[i] << "):\n";
            for (auto &pair : freq[cls.first]) {
                condProb[headers[i]][cls.first][pair.first] =
                    (double)pair.second / classCount[cls.first];
                cout << "P(" << pair.first << " | " << cls.first << ") = "
                     << pair.second << "/" << classCount[cls.first]
                     << " = " << condProb[headers[i]][cls.first][pair.first] << endl;
            }
        }
    }

    // Get test case input
    cout << "\n=== ENTER TEST CASE ===\n";
    map<string, string> test;
    for (int i = 0; i < featureCount; i++) {
        cout << headers[i] << ": ";
        cin >> test[headers[i]];
    }

    cout << "\n=== POSTERIOR PROBABILITIES ===\n";
    map<string, double> posterior;

    for (auto &cls : classCount) {
        double prob = prior[cls.first];
        cout << "\nFor class = " << cls.first << ":\n";
        cout << "Start with prior P(" << cls.first << ") = " << prob << endl;

        for (int i = 0; i < featureCount; i++) {
            string feat = headers[i], val = test[feat];
            double cond = condProb[feat][cls.first].count(val)
                              ? condProb[feat][cls.first][val]
                              : 0.0;
            cout << "P(" << val << " | " << cls.first << ") = " << cond << endl;
            prob *= cond;
        }

        posterior[cls.first] = prob;
        cout << "Final P(" << cls.first << " | Case) = " << prob << endl;
    }

    cout << "\n=== COMPARISON ===\n";
    for (auto &p : posterior)
        cout << "P(" << p.first << " | Case) = " << p.second << endl;

    string prediction = max_element(posterior.begin(), posterior.end(),
                                    [](auto &a, auto &b) {
                                        return a.second < b.second;
                                    })
                            ->first;

    cout << "\nPredicted Class = " << prediction << endl;

    return 0;
}
// ==================================================================================================
// ðŸ”¹ DETAILED EXPLANATION OF NAÃVE BAYES CLASSIFIER PROGRAM
// ==================================================================================================
//
// ðŸ§© PURPOSE:
// This program implements the **NaÃ¯ve Bayes Classifier**, a fundamental **supervised learning**
// technique used for **classification** in Data Mining and Machine Learning.
//
// The classifier uses **Bayesâ€™ Theorem** and assumes that all features (attributes) are conditionally
// independent given the class label.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 1ï¸âƒ£ FORMULA AND THEORY BEHIND NAÃVE BAYES
// --------------------------------------------------------------------------------------------------
//
// Bayesâ€™ Theorem:
//       P(C | X) = [ P(X | C) * P(C) ] / P(X)
//
// In practice, since P(X) is constant for all classes, we only compare the numerators:
//       P(C | X) âˆ P(C) * Î  P(Xi | C)
//
// where:
//   â–ª P(C)      = Prior probability of class C
//   â–ª P(Xi | C) = Conditional probability of feature Xi given class C
//   â–ª P(C | X)  = Posterior probability of class C given all features X
//
// The **class with the highest posterior probability** is selected as the predicted class.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 2ï¸âƒ£ FUNCTION OVERVIEW
// --------------------------------------------------------------------------------------------------
//
// âž¤ split()
//     - Splits a line of CSV data using the comma (`,`) delimiter.
//     - Returns a vector of string tokens.
//
// âž¤ main()
//     - Reads dataset from CSV file.
//     - Calculates prior probabilities P(Class).
//     - Calculates conditional probabilities P(Attribute=value | Class).
//     - Reads a test case (unseen data record).
//     - Computes posterior probabilities for each class using NaÃ¯ve Bayes formula.
//     - Predicts the class with the highest posterior probability.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 3ï¸âƒ£ STEP-BY-STEP LOGICAL FLOW
// --------------------------------------------------------------------------------------------------
//
// STEP 1ï¸âƒ£ â†’ INPUT DATA
//     - The program reads the dataset (CSV format).
//     - The **last column** is assumed to be the target class label.
//     - The rest of the columns are input features.
//
// STEP 2ï¸âƒ£ â†’ CALCULATE PRIOR PROBABILITIES
//     - Count how many records belong to each class.
//     - Compute: P(Class) = (Count of Class) / (Total Records)
//
// Example Output:
//     P(Yes) = 9/14 = 0.643
//     P(No)  = 5/14 = 0.357
//
// STEP 3ï¸âƒ£ â†’ CALCULATE CONDITIONAL PROBABILITIES
//     - For each attribute and each possible value, count how often it appears
//       in combination with each class label.
//     - Compute: P(Attribute=value | Class) = (Count of value in class) / (Count of class)
//
// Example Output:
//     For Class = Yes (Outlook):
//         P(Sunny | Yes) = 2/9 = 0.222
//         P(Rainy | Yes) = 3/9 = 0.333
//
//     For Class = No (Outlook):
//         P(Sunny | No) = 3/5 = 0.6
//         P(Rainy | No) = 2/5 = 0.4
//
// STEP 4ï¸âƒ£ â†’ INPUT TEST CASE
//     - User provides a new record (test case) with feature values.
//     - Example:
//         Outlook: Sunny
//         Humidity: High
//         Windy: False
//
// STEP 5ï¸âƒ£ â†’ COMPUTE POSTERIOR PROBABILITIES FOR EACH CLASS
//     - For each class C:
//           Posterior(C) = P(C) Ã— Î  P(Attribute=value | C)
//
// Example Computation:
//
//     For Class = Yes:
//         P(Yes) Ã— P(Sunny | Yes) Ã— P(High | Yes) Ã— P(False | Yes)
//         = 0.643 Ã— 0.222 Ã— 0.444 Ã— 0.667 = 0.0426
//
//     For Class = No:
//         P(No) Ã— P(Sunny | No) Ã— P(High | No) Ã— P(False | No)
//         = 0.357 Ã— 0.6 Ã— 0.4 Ã— 0.5 = 0.0428
//
// STEP 6ï¸âƒ£ â†’ SELECT CLASS WITH MAXIMUM POSTERIOR
//     - Compare posterior probabilities and pick the higher one.
//
//     Example Output:
//         P(Yes | Case) = 0.0426
//         P(No | Case) = 0.0428
//         Predicted Class = No
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 4ï¸âƒ£ SAMPLE OUTPUT FORMAT
// --------------------------------------------------------------------------------------------------
//
// --- Dataset Loaded Successfully ---
// Total records: 14
// Features: 4
// Target (Class): Play
//
// === PRIOR PROBABILITIES ===
// P(Yes) = 9/14 = 0.643
// P(No)  = 5/14 = 0.357
//
// === CONDITIONAL PROBABILITIES ===
// For Class = Yes (Outlook):
// P(Sunny | Yes) = 2/9 = 0.222
// P(Overcast | Yes) = 4/9 = 0.444
// P(Rainy | Yes) = 3/9 = 0.333
//
// For Class = No (Outlook):
// P(Sunny | No) = 3/5 = 0.6
// P(Overcast | No) = 0/5 = 0
// P(Rainy | No) = 2/5 = 0.4
//
// === ENTER TEST CASE ===
// Outlook: Sunny
// Temperature: Hot
// Humidity: High
// Windy: False
//
// === POSTERIOR PROBABILITIES ===
// P(Yes | Case) = 0.0426
// P(No | Case)  = 0.0428
//
// Predicted Class = No
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 5ï¸âƒ£ INTERNAL VARIABLES USED
// --------------------------------------------------------------------------------------------------
//
// filename          â†’ Input dataset file.
// headers[]         â†’ Attribute names (first row of CSV).
// data[][]          â†’ All dataset rows (vector of vectors).
// classCount{}      â†’ Count of each target class (used for prior probabilities).
// condProb{}        â†’ Nested map for storing conditional probabilities P(Attribute=value | Class).
// test{}            â†’ Stores user input for test record.
// posterior{}       â†’ Stores computed posterior probabilities for each class.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 6ï¸âƒ£ WHY NAÃVE BAYES CLASSIFIER WAS USED (JUSTIFICATION)
// --------------------------------------------------------------------------------------------------
//
// ðŸ”¹ Dataset Type:
//     - The dataset contains **categorical attributes** (e.g., Outlook, Windy, etc.)
//       and a **categorical target label** (e.g., Play = Yes/No).
//
// ðŸ”¹ Objective:
//     - To **classify unseen test cases** into one of the known classes based on probabilities.
//
// ðŸ”¹ Why NaÃ¯ve Bayes is Suitable:
//     1ï¸âƒ£ Works efficiently with categorical data.
//     2ï¸âƒ£ Based on solid probabilistic foundations (Bayesâ€™ Theorem).
//     3ï¸âƒ£ Requires small amounts of training data.
//     4ï¸âƒ£ Simple to implement and interpret.
//     5ï¸âƒ£ Performs surprisingly well even with the â€œindependenceâ€ assumption.
//
// ðŸ”¹ Comparison with Other Methods:
//     - **Decision Tree (ID3/CART):** More complex, requires entropy or Gini calculations.
//     - **K-Means / DBSCAN:** Unsupervised; do not perform classification.
//     - âœ… **NaÃ¯ve Bayes:** Perfect for small, labeled, categorical datasets for classification.
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 7ï¸âƒ£ ADVANTAGES
// --------------------------------------------------------------------------------------------------
//
// âœ… Fast, simple, and efficient â€” works well with large datasets.
// âœ… Performs well even with small amounts of training data.
// âœ… Handles multiple categorical features easily.
// âœ… Requires no parameter tuning (unlike KNN or SVM).
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 8ï¸âƒ£ LIMITATIONS
// --------------------------------------------------------------------------------------------------
//
// âš ï¸ Assumes independence between features â€” often unrealistic in real-world data.
// âš ï¸ Zero probability problem (if an unseen attribute value occurs) â€”
//     can be solved using **Laplace smoothing** (not implemented here).
// âš ï¸ Works best with categorical data (numeric data requires discretization or Gaussian NB).
//
// --------------------------------------------------------------------------------------------------
// ðŸ”¸ 9ï¸âƒ£ CONCLUSION
// --------------------------------------------------------------------------------------------------
//
// âž¤ The NaÃ¯ve Bayes Classifier successfully predicts the class of unseen instances
//     using probabilities derived from training data.
//
// âž¤ It calculates:
//     - Prior probabilities of each class (P(C))
//     - Conditional probabilities of features given class (P(Xi | C))
//     - Posterior probabilities for each class (P(C | X))
//
// âž¤ The class with the **maximum posterior probability** is selected as the prediction.
//
// Example Conclusion Statement:
//     â€œBased on NaÃ¯ve Bayes computation, the test case was classified as â€˜Noâ€™
//      since it had the highest posterior probability.â€
//
// --------------------------------------------------------------------------------------------------
// âœ… FINAL REMARK:
// This experiment demonstrates **Classification using the NaÃ¯ve Bayes Algorithm**,
// which applies Bayesâ€™ Theorem with conditional independence assumptions.
// It is a simple yet powerful probabilistic model widely used in spam filtering,
// medical diagnosis, and text classification.
//
// ==================================================================================================
